# ⏰ 자동화 크롤링 스케줄 시스템 (Automated Crawling Schedules)

## 🎯 **스케줄링 전략 개요**

### **3계층 스케줄 아키텍처**
- **🔥 실시간 모니터링**: 중요 사이트 10분 간격
- **📅 정기 수집**: 일반 소스 매일 오전 6시  
- **📊 심화 분석**: 주간/월간 종합 분석

## ⚡ **실시간 모니터링 (매 10분)**

### **긴급 의료 정보 감시**
```bash
# 🚨 응급의학 & 감염병 모니터링
claude --debug --print "Playwright로 질병관리청 감염병 알림, 응급의료정보센터를 10분마다 체크하고 변화 감지 시 즉시 알림"

# ⚠️ 의약품 안전성 정보
claude --print "식약처 의약품 안전성 서한, 리콜 정보를 실시간 모니터링하고 Sequential로 긴급도 분석"

# 🏥 주요 병원 공지사항
claude --debug --print "Context7에서 주요 대학병원 목록을 확인하고, 중요 공지사항을 실시간으로 추적"
```

### **실시간 스케줄 설정**
```bash
# Windows 작업 스케줄러 연동
claude --print "Desktop을 사용해서 Windows 작업 스케줄러에 10분 간격 크롤링 작업 등록"

# 시스템 리소스 고려 스케줄링
claude --debug --print "Sequential로 시스템 부하를 모니터링하고, 부하가 높을 때는 스케줄을 자동 조정"
```

## 🌅 **일일 정기 수집 (매일 오전 6시)**

### **의료 뉴스 종합 수집**
```bash
# 🗞️ 의료 전문지 일괄 수집
claude --debug --print "Playwright로 메디컬타임즈, 청년의사, 데일리메디, 의학신문 등을 순차적으로 수집"

# 📰 일반 언론 의료 섹션
claude --print "Sequential로 조선일보, 동아일보, 한겨레 의료/건강 섹션을 체계적으로 수집"

# 🌐 해외 의료 뉴스 (영문)
claude --debug --print "Context7에서 주요 해외 의료 뉴스 사이트를 찾고, 국제 의료 동향 수집"
```

### **학술 정보 수집**
```bash
# 📚 국내 의학 저널
claude --print "대한의학회, 대한내과학회 등 주요 학회지의 새로운 논문 정보 수집"

# 🔬 PubMed 키워드 검색
claude --debug --print "사전 설정된 의료 키워드로 PubMed에서 최신 논문 검색 및 메타데이터 수집"

# 📖 임상 가이드라인 업데이트
claude --print "Context7에서 국제 임상 가이드라인 사이트를 확인하고, 업데이트된 지침 수집"
```

### **정부/공공기관 정보**
```bash
# 🏛️ 보건복지부 정책 정보
claude --debug --print "Sequential로 보건복지부, 질병관리청의 정책 발표와 보도자료 수집"

# 💊 건강보험심사평가원 급여 정보
claude --print "심평원의 약가 변경, 급여 기준 변화 등을 자동으로 수집하고 정리"

# 📋 의료기관 평가 정보
claude --debug --print "병원 평가 결과, 의료진 면허 관련 정보 등을 정기적으로 업데이트"
```

## 📊 **주간 종합 분석 (매주 일요일 새벽 2시)**

### **트렌드 분석 및 리포트 생성**
```bash
# 📈 주간 의료 트렌드 분석
claude --debug --print "Sequential로 한 주간 수집된 데이터를 분석해서 주요 트렌드와 키워드 도출"

# 📊 데이터 품질 평가
claude --print "수집된 데이터의 품질을 평가하고, 개선이 필요한 소스나 방법론 식별"

# 📝 주간 리포트 자동 생성
claude --debug --print "Notion을 활용해서 의료진을 위한 주간 종합 리포트를 자동 생성하고 배포"
```

### **시스템 최적화**
```bash
# 🔧 크롤링 성능 최적화
claude --print "Context7에서 크롤링 최적화 방법을 찾고, 느린 사이트나 비효율적인 프로세스 개선"

# 🗃️ 데이터 정리 및 아카이빙
claude --debug --print "Desktop을 사용해서 오래된 데이터를 아카이브하고, 스토리지 공간 최적화"
```

## 🗓️ **월간 심화 분석 (매월 첫째 주 일요일)**

### **종합 성과 분석**
```bash
# 📋 월간 KPI 리포트
claude --debug --print "Sequential로 월간 크롤링 성과를 분석: 수집량, 성공률, 데이터 품질 등"

# 🔄 소스 효율성 평가
claude --print "각 데이터 소스의 가치와 비용을 평가하고, 비효율적인 소스 제거 또는 개선 방안 제시"

# 📊 사용자 만족도 분석
claude --debug --print "수집된 데이터의 실제 활용도를 분석하고, 의료진 피드백을 반영한 개선 계획 수립"
```

## 🕐 **시간대별 스케줄 매트릭스**

### **새벽 시간대 (00:00 - 06:00)**
```bash
# 02:00 - 주간 종합 분석 (일요일)
claude --debug --print "시스템 부하가 낮은 새벽 시간에 데이터 집약적인 분석 작업 실행"

# 03:00 - 데이터베이스 백업 및 정리
claude --print "Desktop으로 데이터 백업, 임시 파일 정리, 시스템 최적화 작업 실행"

# 04:00 - 해외 사이트 수집 (시차 고려)
claude --debug --print "미국/유럽 의료 사이트의 업데이트 시간을 고려한 해외 정보 수집"
```

### **오전 시간대 (06:00 - 12:00)**
```bash
# 06:00 - 일일 뉴스 수집 시작
claude --print "의료진이 출근하기 전 최신 의료 뉴스와 중요 정보 수집 완료"

# 08:00 - 전일 수집 결과 리포트 생성
claude --debug --print "Notion에 전날 수집된 중요 정보를 요약한 일일 브리핑 리포트 생성"

# 10:00 - 실시간 모니터링 강화
claude --print "의료진 활동 시간에 맞춰 실시간 모니터링 주기를 10분에서 5분으로 단축"
```

### **오후 시간대 (12:00 - 18:00)**
```bash
# 14:00 - 학술 정보 집중 수집
claude --debug --print "학회 웹사이트와 논문 데이터베이스에서 새로운 연구 결과 수집"

# 16:00 - 정부 기관 보도자료 수집
claude --print "보건복지부, 식약처 등에서 오후에 발표되는 보도자료와 정책 정보 수집"
```

### **저녁/야간 시간대 (18:00 - 24:00)**
```bash
# 20:00 - 일일 데이터 품질 검증
claude --debug --print "Sequential로 하루 수집된 데이터의 품질을 검증하고 오류 데이터 정정"

# 22:00 - 다음날 크롤링 계획 업데이트
claude --print "Context7에서 새로운 소스나 변경된 사이트 구조를 확인하고 크롤링 전략 업데이트"
```

## 🎯 **특별 이벤트 기반 스케줄링**

### **응급 상황 대응**
```bash
# 🚨 의료 위기상황 감지
claude --debug --print "Sequential로 '집단감염', '의료사고', '약물 부작용' 등 키워드 급증 시 크롤링 주기를 1분으로 단축"

# ⚡ 긴급 알림 시스템
claude --print "중요도가 높은 의료 정보 발견 시 즉시 관련 의료진에게 푸시 알림 발송"
```

### **계절별/주기별 조정**
```bash
# 🦠 감염병 유행 시기
claude --debug --print "독감, 코로나19 등 감염병 유행 시기에 관련 정보 수집 빈도 증가"

# 📅 학회 시즌 대응
claude --print "주요 의학 학회 개최 기간에 발표 자료와 초록 집중 수집"

# 🗳️ 의료 정책 변화 시기
claude --debug --print "새 정부 출범, 건강보험 정책 변화 시기에 관련 정보 모니터링 강화"
```

## 📈 **스케줄 성능 최적화**

### **동적 스케줄 조정**
```bash
# 📊 성능 기반 자동 조정
claude --debug --print "Sequential로 각 사이트의 응답 시간과 데이터 품질을 모니터링하고 스케줄 자동 최적화"

# 🔄 부하 분산 스케줄링
claude --print "시스템 리소스 사용량에 따라 크롤링 작업을 시간대별로 분산 배치"

# 📉 실패율 기반 재조정
claude --debug --print "특정 사이트의 크롤링 실패율이 높으면 접근 빈도를 줄이고 복구 후 재개"
```

### **효율성 측정 지표**
| 시간대 | 대상 사이트 수 | 수집 목표량 | 성공률 목표 | 평균 소요시간 |
|--------|-------------|-----------|-----------|-------------|
| **실시간** | 10개 핵심 사이트 | 50개 항목 | >95% | <5분 |
| **일일** | 50개 사이트 | 500개 항목 | >90% | <2시간 |
| **주간** | 전체 소스 | 전량 분석 | >85% | <4시간 |
| **월간** | 성과 분석 | 종합 리포트 | >95% | <6시간 |

## 🔧 **스케줄 관리 도구**

### **스케줄러 설정 및 관리**
```bash
# Windows 작업 스케줄러 통합
claude --debug --print "Desktop을 통해 Windows 작업 스케줄러에 모든 크롤링 스케줄을 등록하고 중앙 관리"

# 로그 및 모니터링
claude --print "Sequential로 모든 스케줄 실행 결과를 로깅하고, 실행 상태를 실시간으로 모니터링"

# 수동 제어 인터페이스
claude --debug --print "Notion 대시보드를 통해 특정 스케줄을 일시정지, 재시작, 수정할 수 있는 제어 인터페이스 구축"
```

이제 완전 자동화된 크롤링 스케줄 시스템이 구축되어 24/7 의료 정보 수집이 가능합니다! ⏰✨